sectionId,toolId,name,scriptPath,workflowResultType,groundTruthFormat,groundTruthSubcategory,dependencyParamName,description
audio_segmentation,ina_precision_segment,INA Speech Segmenter Precision/Recall (by segments),ina_precision_segment.py,segment,csv,segment_timecodes_labels,,"<p>This test inputs a ground truth of timestamp/label annotations for an audio recording using the INA Speech Segmenter labels--speech, music, noise, silence. Ranges of silence or noise should only be recorded if they are longer than 10 seconds. Otherwise, they should be included with the previous segment. Notes may be included in a separate column in the ground truth to assist with qualitative review--these will not be incorporated into the scoring. Accuracy (for all segments and by segment type), precision, recall and F1 are calculated by segment.</p>
<p>Ground truth: CSV with a minimum of three columns--start, end, label. Values for start and end should be recorded as hh:mm:ss or in seconds (with decimal). For best results, segments should start at the end of the previous one (ex. If a segment of speech ends at 00:45:12, the next segment should start at 00:45:12.)</p>"
audio_segmentation,ina_precision_second,INA Speech Segmenter Precision/Recall (by seconds),ina_precision_second.py,segment,csv,segment_timecodes_labels,,"<p>This test inputs a ground truth of timestamp/label annotations for an audio recording using the INA Speech Segmenter labels--speech, music, noise, silence. Similar to the precision/recall test by segments, but accuracy, precision, recall, and F1 are calculated by seconds (i.e. comparing classification for each second rather than comparing segments of contiguous classifications). This test inputs a ground truth of timestamp/label annotations for an audio recording using the INA Speech Segmenter labels--speech, music, noise, silence. Ranges of silence or noise should only be recorded if they are longer than 10 seconds. Otherwise, they should be included with the previous segment. Notes may be included in a separate column in the ground truth to assist with qualitative review--these will not be incorporated into the scoring.</p>
<p>Ground truth: CSV with a minimum of three columns--start, end, label. Values for start and end should be recorded as hh:mm:ss or in seconds (with decimal). For best results, segments should start at the end of the previous one (ex. If a segment of speech ends at 00:45:12, the next segment should start at 00:45:12.)</p> "
applause_detection,applause_precision_segment,Precision/Recall of Applause/Non-applause and Time Codes,applause_precision.py,segment,csv,applause_timecodes_labels,,"<p>This test inputs a structured list of timestamp ranges and labels (“applause” or “non-applause”)  and compares it to the Applause Detection output to find true positives, false positives, and false negatives, matching on both time ranges and labels.</p>
<p>Ground truth: CSV with a minimum of three columns--start, end, label (applause/non-applause). Values for start and end should be recorded as hh:mm:ss or in seconds (with decimal).</p>"
applause_detection,applause_precision_second,Precision/Recall of Applause/Non-applause (by second),applause_precision.py,segment,csv,applause_timecodes_labels,,"<p>This test inputs a structured list of timestamp ranges and labels (“applause” or “non-applause”)  and compares it to the Applause Detection output to find true positives, false positives, and false negatives, matching on both time ranges and labels.</p>
<p>Ground truth: CSV with a minimum of three columns--start, end, label (applause/non-applause). Values for start and end should be recorded as hh:mm:ss or in seconds (with decimal).</p>"
speech_to_text,stt_word_error_rate,Word Error Rate,stt_word_error_rate.py,transcript,txt,stt_transcript,,"<p>Word error rate measures speech-to-text accuracy by comparing the generated transcript against a ground truth transcription and calculating the number of errors (substitutions, additions, and deletions) as the cost of restoring the output word sequence to the original input sequence. Scores are measured as percentages, with lower scores representing high accuracy (low word error rate). Scores may exceed 100%, especially if the STT engine produced many insertions. Related to WER, character error rate (CER) is similar to WER, but based on characters, the word information loss (WIL) measures the proportion of word information lost in a transcription, and word information processed (WIP) measures the inverse of WIL.</p>
<p>Ground truth: Plain text (.txt) transcript of spoken word audio.</p>"
named_entity_recognition,ner_precision_allEntity_perTool,Precision/Recall of All Entity Instances (MGM-specific),ner_precision_allEntity_perTool.py,ner,csv,"{""Amazon Comprehend"": ""ner_all-aws"", ""SpaCy"": ""ner_all_spacy""}",MGM being tested,"<p>This test inputs a structured list of character offsets, entities, and types for outputs from one NER MGM (Amazon Comprehend or SpaCy) and compares it to a ground truth to find true positives, false positives, and false negatives, matching on character offsets, entities, and types.</p>
<p>Ground truth: CSV with a minimum of four columns--start, end, text, and type–labeled with MGM-specific entity types. (See wiki documentation for all each MGM's entity types.)</p>
"
named_entity_recognition,ner_precision_allEntity_mapped,Precision/Recall of All Entity Instances (mapped),ner_precision_allEntity_mapped.py,ner,csv,"{""Amazon Comprehend"": ""ner_all-aws"", ""SpaCy"": ""ner_all_spacy"", ""Common"": ""ner_all_common""}",,"<p>This test inputs a structured list of character offsets, entities, and types and compares it to the NER output to find true positives, false positives, and false negatives, matching on character offsets, entities, and commonly mapped types. Because each NER MGM uses its own list of entity types, this test is useful for comparing different MGMs.</p>
<p>Ground truth: CSV with a minimum of four columns--start, end, text, and type–labeled with the following entity types OR entity types from one of the available MGMs (which will be automatically mapped to this common entity set):</p>
<p><ul>
<li>CONCEPT</li>
<li>EVENT</li>
<li>LOCATION</li>
<li>ORGANIZATION</li>
<li>PERSON</li>
<li>DATE</li>
<li>NUMBER</li>
</ul></p>"
named_entity_recognition,ner_precision_uniqueEntity_perTool,Precision/Recall of Unique Entities (tool-specific),ner_precision_uniqueEntity_perTool.py,ner,csv,"{""Amazon Comprehend"": ""ner_unique-aws"", ""SpaCy"": ""ner_unique_spacy""}",MGM being tested,"<p>This test inputs a structured list of unique entities and their types for outputs from one NER MGM (Amazon Comprehend or SpaCy) and compares it to a ground truth output to find true positives, false positives, and false negatives, matching on entities and types.</p>
<p>Ground truth: CSV with a minimum of two columns-- text and type–labeled with MGM-specific entity types. (See wiki documentation for all each MGM's entity types.)</p>"
named_entity_recognition,ner_precision_uniqueEntity_mapped,Precision/Recall of Unique Entities (mapped),ner_precision_uniqueEntity_mapped.py,ner,csv,"{""Amazon Comprehend"": ""ner_unique-aws"", ""SpaCy"": ""ner_unique_spacy"", ""Common"": ""ner_unique_common""}",,"<p>This test inputs a structured list of unique entities and their types and compares it to the NER output to find true positives, false positives, and false negatives, matching on entities and commonly mapped types. Because each NER MGM uses its own list of entity types, this test is useful for comparing different tools.</p>
<p>Ground truth: CSV with a minimum of two columns-- text and type–labeled with the following entity types OR entity types from one of the available MGMs (which will be automatically mapped to this common entity set):</p>
<p><ul>
<li>CONCEPT</li>
<li>EVENT</li>
<li>LOCATION</li>
<li>ORGANIZATION</li>
<li>PERSON</li>
<li>DATE</li>
<li>NUMBER</li>
</ul></p>"
shot_detection,shot_precision,Precision/Recall of Shots,shot_precision.py,shot,csv,shots_timecodes_labels,,"<p>This test inputs a structured list of timestamp ranges representing the start and end of each shot and compares it to the shot detection output to find true positives, false positives, and false negatives, accounting for a threshold of seconds before and after each transition.</p>
<p>Ground truth: CSV with two columns–start and end. Values for start and end should be recorded as hh:mm:ss or in seconds (with decimal). For best results, shots should start at the end of the previous one (ex. If a shot ends at 00:45:12, the next shot should start at 00:45:12.)</p>"
video_ocr,vocr_precision_text,Precision/Recall of Texts,vocr.py,vocr,csv,vocr_texts,,"<p>This test inputs a structured list of texts and compares it to the Video OCR output to find true positives, false positives, and false negatives, matching on a list of texts to see where text was detected. This is useful if you don’t want to record timestamps.</p>
<p>Ground truth: CSV files with list of texts and, optionally, the number of occurrences in the video.</p>"
video_ocr,vocr_precision_text_unique,Precision/Recall of Unique Texts,vocr.py,vocr,csv,vocr_unique_texts,,"<p>This test inputs a structured list of texts and compares it to the Video OCR output to find true positives, false positives, and false negatives, matching on a unique list of texts to see where text was detected. This is useful if you don’t care when the text appears in the video.</p>
<p>Ground truth: CSV files with list of texts and, optionally, the number of occurrences in the video.</p>"