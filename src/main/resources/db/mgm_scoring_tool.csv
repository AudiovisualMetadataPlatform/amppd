sectionId,toolId,name,scriptPath,workflowResultType,groundTruthFormat,description
audio_segmentation,ina_precision_segment,INA Speech Segmenter Precision/Recall (by segments),ina_precision_segment.py,segment,csv,"<p>This test inputs a ground truth of timestamp/label annotations for an audio recording using the INA Speech Segmenter labels--speech, music, noise, silence. Ranges of silence or noise should only be recorded if they are longer than 10 seconds. Otherwise, they should be included with the previous segment. Notes may be included in a separate column in the ground truth to assist with qualitative review--these will not be incorporated into the scoring. Accuracy (for all segments and by segment type), precision, recall and F1 are calculated by segment.</p>
<p>Ground truth: CSV with a minimum of three columns--start, end, label. Values for start and end should be recorded as hh:mm:ss or in seconds (with decimal). For best results, segments should start at the end of the previous one (ex. If a segment of speech ends at 00:45:12, the next segment should start at 00:45:12.)</p>"
audio_segmentation,ina_precision_second,INA Speech Segmenter Precision/Recall (by seconds),ina_precision_second.py,segment,csv,"<p>Similar to the precision/recall test by segments, but accuracy, precision, recall, and F1 are calculated by seconds (i.e. comparing classification for each second rather than comparing segments of contiguous classifications). This test inputs a ground truth of timestamp/label annotations for an audio recording using the INA Speech Segmenter labels--speech, music, noise, silence. Ranges of silence or noise should only be recorded if they are longer than 10 seconds. Otherwise, they should be included with the previous segment. Notes may be included in a separate column in the ground truth to assist with qualitative review--these will not be incorporated into the scoring.</p>
<p>Ground truth: CSV with a minimum of three columns--start, end, label. Values for start and end should be recorded as hh:mm:ss or in seconds (with decimal). For best results, segments should start at the end of the previous one (ex. If a segment of speech ends at 00:45:12, the next segment should start at 00:45:12.)</p> "
applause_detection,applause_precision,Precision/Recall of Applause/Non-applause and Time Codes,applause_precision.py,segment,csv,"<p>This test inputs a structured list of timestamp ranges and labels (“applause” or “non-applause”)  and compares it to the Applause Detection output to find true positives, false positives, and false negatives, matching on both time ranges and labels.</p>
<p>Ground truth: CSV with a minimum of three columns--start, end, label (applause/non-applause). Values for start and end should be recorded as hh:mm:ss or in seconds (with decimal).</p>"
speech_to_text,stt_word_error_rate,Word Error Rate,stt_word_error_rate.py,transcript,txt,"<p>Word error rate measures speech-to-text accuracy by comparing the generated transcript against a ground truth transcription and calculating the number of errors (substitutions, additions, and deletions) as the cost of restoring the output word sequence to the original input sequence. Scores are measured as percentages, with lower scores representing high accuracy (low word error rate). Scores may exceed 100%, especially if the STT engine produced many insertions. Related to WER, character error rate (CER) is similar to WER, but based on characters, the word information loss (WIL) measures the proportion of word information lost in a transcription, and word information processed (WIP) measures the inverse of WIL.</p>
<p>Ground truth: Plain text (.txt) transcript of spoken word audio.</p>"
named_entity_recognition,ner_precision_allEntity_perTool,Precision/Recall of All Entity Instances (tool-specific),ner_precision_allEntity_perTool.py,ner,csv,"<p>This test inputs a structured list of character offsets, entities, and types for outputs from one tool (Amazon Comprehend or SpaCy) and compares it to the NER output to find true positives, false positives, and false negatives, matching on character offsets, entities, and types.</p>
<p>Ground truth: CSV with a minimum of four columns--start, end, text, and type–labeled with tool-specific entity types.</p>
"
named_entity_recognition,ner_precision_allEntity_mapped,Precision/Recall of All Entity Instances (mapped),ner_precision_allEntity_mapped.py,ner,csv,"<p>This test inputs a structured list of character offsets, entities, and types and compares it to the NER output to find true positives, false positives, and false negatives, matching on character offsets, entities, and commonly mapped types. Because each NER tool uses its own list of entity types, this test is useful for comparing different tools.</p>
<p>Ground truth: CSV with a minimum of four columns--start, end, text, and type–labeled with the following entity types:</p>
<p><ul>
<li>CONCEPT</li>
<li>EVENT</li>
<li>LOCATION</li>
<li>ORGANIZATION</li>
<li>PERSON</li>
<li>DATE</li>
</ul></p>
<p>Parameters:</p>
<p>(optional) Entity types to include in scoring. If left blank, then all entities will be included. </p>"
named_entity_recognition,ner_precision_uniqueEntity_perTool,Precision/Recall of Unique Entities (tool-specific),ner_precision_uniqueEntity_perTool.py,ner,csv,"<p>This test inputs a structured list of unique entities and their types for outputs from one tool (Amazon Comprehend or SpaCy) and compares it to the NER output to find true positives, false positives, and false negatives, matching on entities and types.</p>
<p>Ground truth: CSV with a minimum of two columns-- text and type–labeled with tool-specific entity types.</p>"
named_entity_recognition,ner_precision_uniqueEntity_mapped,Precision/Recall of Unique Entities (mapped),ner_precision_uniqueEntity_mapped.py,ner,csv,"<p>This test inputs a structured list of unique entities and their types and compares it to the NER output to find true positives, false positives, and false negatives, matching on entities and commonly mapped types. Because each NER tool uses its own list of entity types, this test is useful for comparing different tools.</p>
<p>Ground truth: CSV with a minimum of two columns-- text and type–labeled with the following entity types:</p>
<p><ul>
<li>CONCEPT</li>
<li>EVENT</li>
<li>LOCATION</li>
<li>ORGANIZATION</li>
<li>PERSON</li>
<li>DATE</li>
</ul></p>"
shot_detection,shot_precision,Precision/Recall of Shots,shot_precision.py,shot,csv,"<p>This test inputs a structured list of timestamp ranges representing the start and end of each shot and compares it to the shot detection output to find true positives, false positives, and false negatives, accounting for a threshold of seconds before and after each transition.</p>
<p>Ground truth: CSV with two columns–start and end. Values for start and end should be recorded as hh:mm:ss or in seconds (with decimal). For best results, shots should start at the end of the previous one (ex. If a shot ends at 00:45:12, the next shot should start at 00:45:12.)</p>"
facial_recognition,face_precision_appearance,Precision/Recall of Face Appearances,face_precision_appearance.py,face,csv,"<p>This test inputs a structured list of timestamp ranges and labels and compares it to the Face Recognition output to find true positives, false positives, and false negatives, matching on both time ranges and labels.</p>
<p>Ground truth: CSV with a minimum of three columns--start, end, label. Values for start and end should be recorded as hh:mm:ss or in seconds (with decimal). </p>"
facial_recognition,face_precision_label,Precision/Recall of Face Labels,face_precision_label.py,face,csv,"<p>This test inputs a list of unique name labels of people appearing in a video and compares it to the Face Recognition output to find true positives, false positives, and false negatives, matching on unique labels.</p>
<p>Ground truth: CSV with a list of unique name labels, one per row.</p>"
video_ocr,vocr_precision_text_time,Precision/Recall of Texts and Timestamps,vocr_precision_text_time.py,vocr,csv,"<p>This test inputs a structured list of timestamp ranges and texts and compares it to the Video OCR output to find true positives, false positives, and false negatives, matching on both time ranges and texts.</p>
<p>Ground truth: CSV with a minimum of three columns--start, end, text. Values for start and end should be recorded as hh:mm:ss or in seconds (with decimal).</p>"
video_ocr,vocr_precision_time,Precision/Recall of Texts Detected (Matching Timestamps),vocr_precision_time.py,vocr,csv,"<p>This test inputs a structured list of timestamp ranges and texts and compares it to the Video OCR output to find true positives, false positives, and false negatives, matching on just time ranges to see where text was detected. This is useful if you expect text to be inaccurate but will be using contact sheets to verify a correct text but want to make sure that all occurrences of text are detected. </p>
<p>Ground truth: CSV with a minimum of three columns--start, end, text. Values for start and end should be recorded as hh:mm:ss or in seconds (with decimal).</p>"
video_ocr,vocr_precision_text,Precision/Recall of Texts Detected (Matching Texts),vocr_precision_text.py,vocr,csv,"<p>This test inputs a structured list of timestamp ranges and texts and compares it to the Video OCR output to find true positives, false positives, and false negatives, matching on a unique lists of texts (or a list of texts and number of occurrences) to see where text was detected. This is useful if you don’t care where the text appears in the video.</p>
<p>Ground truth: CSV files with list of texts and, optionally, the number of occurrences in the video.</p>"
