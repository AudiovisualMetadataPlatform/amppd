sectionId,toolId,name,scriptPath,workflowResultType,groundTruthFormat,groundTruthSubcategory,dependencyParamName,description,groundTruthTemplate,,workflowResultOutput,visualizationPageDescription
audio_segmentation,ina_precision_segment,INA Speech Segmenter Precision/Recall (by segments),ina_precision_segment.py,segment,csv,segment_timecodes_labels,,"<p>This test inputs a ground truth of timestamp/label annotations for an audio recording using the INA Speech Segmenter labels--speech, music, noise, silence. Ranges of silence or noise should only be recorded if they are longer than 10 seconds. Otherwise, they should be included with the previous segment. Notes may be included in a separate column in the ground truth to assist with qualitative review--these will not be incorporated into the scoring. Accuracy (for all segments and by segment type), precision, recall and F1 are calculated by segment.</p>
<p>Ground truth: CSV with a minimum of three columns--start, end, label. Values for start and end should be recorded as hh:mm:ss or in seconds (with decimal). For best results, segments should start at the end of the previous one (ex. If a segment of speech ends at 00:45:12, the next segment should start at 00:45:12.)</p>",AudioSegmentationGTTemplate.csv,,amp_segments,"Overall precision, recall, F1, and accuracy, as well as accuracy scores for speech, music, noise, and silence segments found by INA Speech Segmenter. Scores are based on comparison of identified audio segments considering a given threshold of seconds before or after each segment. For example, a 2-second threshold parameter will consider a GT and MGM segment a match if both the start and end times for each fall within 2 seconds of each other."
audio_segmentation,ina_precision_second,INA Speech Segmenter Precision/Recall (by seconds),ina_precision_second.py,segment,csv,segment_timecodes_labels,,"<p>This test inputs a ground truth of timestamp/label annotations for an audio recording using the INA Speech Segmenter labels--speech, music, noise, silence. Similar to the precision/recall test by segments, but accuracy, precision, recall, and F1 are calculated by seconds (i.e. comparing classification for each second rather than comparing segments of contiguous classifications). This test inputs a ground truth of timestamp/label annotations for an audio recording using the INA Speech Segmenter labels--speech, music, noise, silence. Ranges of silence or noise should only be recorded if they are longer than 10 seconds. Otherwise, they should be included with the previous segment. Notes may be included in a separate column in the ground truth to assist with qualitative review--these will not be incorporated into the scoring.</p>
<p>Ground truth: CSV with a minimum of three columns--start, end, label. Values for start and end should be recorded as hh:mm:ss or in seconds (with decimal). For best results, segments should start at the end of the previous one (ex. If a segment of speech ends at 00:45:12, the next segment should start at 00:45:12.)</p> ",AudioSegmentationGTTemplate.csv,,amp_segments,"Overall precision, recall, F1, and accuracy, as well as accuracy scores for speech, music, noise, and silence found by INA Speech Segmenter. Scores are based on comparison of classification for each second in the media item."
applause_detection,applause_precision_segment,Precision/Recall of Applause/Non-applause and Time Codes,applause_precision.py,segment,csv,applause_timecodes_labels,,"<p>This test inputs a structured list of timestamp ranges and labels (“applause” or “non-applause”)  and compares it to the Applause Detection output to find true positives, false positives, and false negatives, matching on both time ranges and labels.</p>
<p>Ground truth: CSV with a minimum of three columns--start, end, label (applause/non-applause). Values for start and end should be recorded as hh:mm:ss or in seconds (with decimal).</p>",ApplauseDetectionGTTemplate.csv,,amp_segments,"Overall precision, recall, F1, accuracy, as well as accuracy for applause and non-applause segments found by Applause Detection. Scores are based on comparison of identified audio segments considering a given threshold of seconds before or after each segment. For example, a 2-second threshold parameter will consider a GT and MGM segment a match if both the start and end times for each fall within 2 seconds of each other."
applause_detection,applause_precision_second,Precision/Recall of Applause/Non-applause (by second),applause_precision.py,segment,csv,applause_timecodes_labels,,"<p>This test inputs a structured list of timestamp ranges and labels (“applause” or “non-applause”)  and compares it to the Applause Detection output to find true positives, false positives, and false negatives. Scores are calculated based on comparing the classification for each second, rather than comparing identified segments.</p>
<p>Ground truth: CSV with a minimum of three columns--start, end, label (applause/non-applause). Values for start and end should be recorded as hh:mm:ss or in seconds (with decimal).</p>",ApplauseDetectionGTTemplate.csv,,amp_segments,"Overall precision, recall, F1, accuracy, as well as accuracy scores for applause and non-applause. Scores are based on comparison of classification for each second in the media item."
speech_to_text,stt_word_error_rate,Word Error Rate,stt_word_error_rate.py,transcript,txt,stt_transcript,,"<p>Word error rate measures speech-to-text accuracy by comparing the generated transcript against a ground truth transcription and calculating the number of errors (substitutions, additions, and deletions) as the cost of restoring the output word sequence to the original input sequence. Scores are measured as percentages, with lower scores representing high accuracy (low word error rate). Scores may exceed 100%, especially if the STT engine produced many insertions. Related to WER, character error rate (CER) is similar to WER, but based on characters, the word information loss (WIL) measures the proportion of word information lost in a transcription, and word information processed (WIP) measures the inverse of WIL.</p>
<p>Ground truth: Plain text (.txt) transcript of spoken word audio.</p>",SttGTTemplate.txt,,"amp_transcript_adjusted,amp_transcript_aligned,amp_transcript_corrected,amp_transcript","Word information loss (proportion of word information lost in a transcription) and word information processed (proportion of word information processed), as well as other error rates. Generally lower scores represent higher accuracy (ex: low word error rate). Word information processed is the exception, as a higher score represents more information successfully processed."
named_entity_recognition,ner_precision_allEntity_perTool,Precision/Recall of All Entity Instances (MGM-specific),ner_precision_allEntity_perTool.py,ner,csv,"{""Amazon Comprehend"": ""ner_all-aws"", ""SpaCy"": ""ner_all_spacy""}",MGM being tested,"<p>This test inputs a structured list of character offsets, entities, and types for outputs from one NER MGM (Amazon Comprehend or SpaCy) and compares it to a ground truth to find true positives, false positives, and false negatives, matching on character offsets, entities, and types.</p>
<p>Ground truth: CSV with a minimum of four columns--start, end, text, and type–labeled with MGM-specific entity types. (See wiki documentation for all each MGM's entity types.)</p>
",NerAllGTTemplate.csv,,"amp_entities,amp_entities_corrected","Precision, recall, accuracy, and f1, as well as true positives, false positives, and false negatives. This test uses character offset to compare all entities that occur throughout the media item, requiring they occur very closely in the transcript text to count as a match. Entity labels may or may not be required to match, depending on the parameter chosen."
named_entity_recognition,ner_precision_allEntity_mapped,Precision/Recall of All Entity Instances (mapped),ner_precision_allEntity_mapped.py,ner,csv,"{""Amazon Comprehend"": ""ner_all-aws"", ""SpaCy"": ""ner_all_spacy"", ""Common"": ""ner_all_common""}",Ground truth entities,"<p>This test inputs a structured list of character offsets, entities, and types and compares it to the NER output to find true positives, false positives, and false negatives, matching on character offsets, entities, and commonly mapped types. Because each NER MGM uses its own list of entity types, this test is useful for comparing different MGMs.</p>
<p>Ground truth: CSV with a minimum of four columns--start, end, text, and type–labeled with the following entity types OR entity types from one of the available MGMs (which will be automatically mapped to this common entity set):</p>
<p><ul>
<li>CONCEPT</li>
<li>EVENT</li>
<li>LOCATION</li>
<li>ORGANIZATION</li>
<li>PERSON</li>
<li>DATE</li>
<li>NUMBER</li>
</ul></p>",NerAllGTTemplate.csv,,"amp_entities,amp_entities_corrected","Precision, recall, accuracy, and f1, as well as true positives, false positives, and false negatives. This test uses character offset to compare all entities that occur throughout the media item, requiring they occur very closely in the transcript text to count as a match. Entity labels may or may not be required to match, depending on the parameter chosen. Note that MGM-specific entity labels are mapped to a smaller common subset for comparison."
named_entity_recognition,ner_precision_uniqueEntity_perTool,Precision/Recall of Unique Entities (MGM-specific),ner_precision_uniqueEntity_perTool.py,ner,csv,"{""Amazon Comprehend"": ""ner_unique_aws"", ""SpaCy"": ""ner_unique_spacy""}",MGM being tested,"<p>This test inputs a structured list of unique entities and their types for outputs from one NER MGM (Amazon Comprehend or SpaCy) and compares it to a ground truth output to find true positives, false positives, and false negatives, matching on entities and types.</p>
<p>Ground truth: CSV with a minimum of two columns-- text and type–labeled with MGM-specific entity types. (See wiki documentation for all each MGM's entity types.)</p>",NerUniqueGTTemplate.csv,,"amp_entities,amp_entities_corrected","Precision, recall, accuracy, and f1, as well as true positives, false positives, and false negatives. This test only compares unique occurrences of de-duplicated entities without reference to where they occur in the MGM and ground truth outputs. Entity labels may or may not be required to match, depending on the parameter chosen."
named_entity_recognition,ner_precision_uniqueEntity_mapped,Precision/Recall of Unique Entities (mapped),ner_precision_uniqueEntity_mapped.py,ner,csv,"{""Amazon Comprehend"": ""ner_unique_aws"", ""SpaCy"": ""ner_unique_spacy"", ""Common"": ""ner_unique_common""}",Ground truth entities,"<p>This test inputs a structured list of unique entities and their types and compares it to the NER output to find true positives, false positives, and false negatives, matching on entities and commonly mapped types. Because each NER MGM uses its own list of entity types, this test is useful for comparing different tools.</p>
<p>Ground truth: CSV with a minimum of two columns-- text and type–labeled with the following entity types OR entity types from one of the available MGMs (which will be automatically mapped to this common entity set):</p>
<p><ul>
<li>CONCEPT</li>
<li>EVENT</li>
<li>LOCATION</li>
<li>ORGANIZATION</li>
<li>PERSON</li>
<li>DATE</li>
<li>NUMBER</li>
</ul></p>",NerUniqueGTTemplate.csv,,"amp_entities,amp_entities_corrected","Precision, recall, accuracy, and f1, as well as true positives, false positives, and false negatives. This test only compares unique occurrences of de-duplicated entities without reference to where they occur in the MGM and ground truth outputs. Entity labels may or may not be required to match, depending on the parameter chosen. Note that MGM-specific entity labels are mapped to a smaller common subset for comparison."
shot_detection,shot_precision,Precision/Recall of Shots,shot_precision.py,shot,csv,shots_timecodes_labels,,"<p>This test inputs a structured list of timestamp ranges representing the start and end of each shot and compares it to the shot detection output to find true positives, false positives, and false negatives, accounting for a threshold of seconds before and after each transition.</p>
<p>Ground truth: CSV with two columns–start and end. Values for start and end should be recorded as hh:mm:ss or in seconds (with decimal). For best results, shots should start at the end of the previous one (ex. If a shot ends at 00:45:12, the next shot should start at 00:45:12.)</p>",ShotDetectionGTTemplate.csv,,amp_shots,"Precision, recall, accuracy, and f1, as well as true positives, false positives, and false negatives. Scores are based on comparison of start and end of shots considering a given threshold of seconds before or after each shot. For example, a 2-second threshold parameter will consider a GT and MGM shot a match if both the start and end times for each fall within 2 seconds of each other."
video_ocr,vocr_precision_text_unique,Precision/Recall of Unique Texts,vocr.py,vocr,csv,vocr_unique_texts,,"<p>This test inputs a structured list of texts and compares it to the Video OCR output to find true positives, false positives, and false negatives, matching on a unique list of texts to see where text was detected. This is useful if you don’t care when the text appears in the video.</p>
<p>Ground truth: CSV files with list of texts and, optionally, the number of occurrences in the video.</p>",VOCRTextGTTemplate.csv,,"amp_vocr,amp_vocr_dedupe","Precision, recall, accuracy, and f1, as well as true positives, false positives, and false negatives. Scores are based on comparison of unique occurrences of de-duplicated texts without reference to when they occur in the MGM and ground truth outputs."
